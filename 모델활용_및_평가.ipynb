{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2t8t4DgxDMh5",
        "rSupsPhoDOon"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 패턴인식 설계 과제\n",
        "# <한국어 문장 긍정,부정문 이진분류>\n",
        "## 단국대학교 컴퓨터공학과 정민준\n",
        "jmj284@gmail.com<br> \n",
        "010-9391-0801\n",
        "\n",
        "# <모델 사용 및 테스트 코드>\n",
        "\n",
        "##<학습 및 테스트 환경> <br>\n",
        "본 프로그램은 'Google Colab' 환경에서 작성 되었습니다.<br>\n",
        "플랫폼: 'Goolge Colab'<br> \n",
        "GPU: Tesla T4<br> \n",
        "GPU API: cuda<br>\n",
        "<br>사용한 언어 및 라이브러리는 아래와 같습니다.\n",
        "<br>\n",
        "\n",
        "## <언어> <br>\n",
        "\n",
        "###1. python3\n",
        "Version: 3.7.14 (default, Sep  8 2022, 00:06:44)<br>\n",
        "GCC 7.5.0<br>\n",
        "\n",
        "## <라이브러리> <br>\n",
        "\n",
        "###1. transformers\n",
        "Version: 4.23.1<br> \n",
        "License: Apache<br>\n",
        "\n",
        "###2. torch\n",
        "Version: 1.12.1+cu113<br>\n",
        "License: BSD-3<br>\n",
        "\n",
        "###3. tensorflow)<br>\n",
        "Version: 2.9.2<br>\n",
        "License: Apache 2.0<br>\n",
        "\n",
        "###4. keras\n",
        "Version: 2.9.0<br>\n",
        "License: Apache 2.0<br>\n",
        "\n",
        "###5. scikit-learn\n",
        "Version: 1.0.2<br>\n",
        "License: new BSD<br>\n",
        "\n",
        "###6. pandas\n",
        "Version: 1.3.5<br> \n",
        "License: BSD-3-Clause<br>\n",
        "\n",
        "###7. numpy\n",
        "Version: 1.21.6<br>\n",
        "License: BSD<br>\n",
        "\n",
        "###8. matplotlib\n",
        "Version: 3.2.2<br> \n",
        "License: PSF<br>\n",
        "\n",
        "## <사전학습모델> \n",
        "KoBERT-Transfomers<br>\n",
        "License: Apache 2.0\n",
        "\n",
        "## <참고자료>\n",
        "\n",
        "1. https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "2. https://github.com/monologg/KoBERT-Transformers/blob/master/kobert_transformers/tokenization_kobert.py\n",
        "\n",
        "3. https://velog.io/@seolini43/일상연애-주제의-한국어-대화-BERT로-이진-분류-모델-만들기파이썬Colab-코드"
      ],
      "metadata": {
        "id": "NG6sz9YzIvxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 호출 및 설정"
      ],
      "metadata": {
        "id": "2t8t4DgxDMh5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo3groAXC68n",
        "outputId": "643ea9c5-27e9-4c68-8105-2a5f7b6b919e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1 MB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.9.24)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.21.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.32)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.9)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595734 sha256=94f1f0cf9b0f9307c5205084bca187974197e451823c921c8d06c9a6c7d8bb2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 7.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kobert-transformers\n",
            "  Downloading kobert_transformers-0.5.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from kobert-transformers) (0.1.97)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from kobert-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: transformers<5,>=3 in /usr/local/lib/python3.7/dist-packages (from kobert-transformers) (4.24.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->kobert-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (0.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (0.13.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5,>=3->kobert-transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=3->kobert-transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.0.4)\n",
            "Installing collected packages: kobert-transformers\n",
            "Successfully installed kobert-transformers-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip3 install kobert-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-mTWEOnC-qv",
        "outputId": "b9516213-0aef-4352-98c1-2140491eba8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-1q4ikwnf\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-1q4ikwnf\n",
            "Collecting boto3<=1.15.18\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
            "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.7 MB 25 kB/s \n",
            "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 12.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 47.6 MB/s \n",
            "\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n",
            "  Downloading torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.1 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x397fa000 @  0x7f4055268615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 16 kB/s \n",
            "\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.19.0,>=1.18.18\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 43.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.32)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.1.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n",
            "Building wheels for collected packages: kobert, sacremoses\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=1697cff191e6f58d0e49659dd066e02e21e5bbd5d6948855cd0a444590b7fd05\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2zaojdc1/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=8448b5a2a8e50fe85db3c81d48006b39c706355700e8f516a469914234f6b88f\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built kobert sacremoses\n",
            "Installing collected packages: jmespath, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, transformers, torch, sentencepiece, onnxruntime, mxnet, boto3, kobert\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.2\n",
            "    Uninstalling tokenizers-0.13.2:\n",
            "      Successfully uninstalled tokenizers-0.13.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.11.0\n",
            "    Uninstalling huggingface-hub-0.11.0:\n",
            "      Successfully uninstalled huggingface-hub-0.11.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.24.0\n",
            "    Uninstalling transformers-4.24.0:\n",
            "      Successfully uninstalled transformers-4.24.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.97\n",
            "    Uninstalling sentencepiece-0.1.97:\n",
            "      Successfully uninstalled sentencepiece-0.1.97\n",
            "  Attempting uninstall: mxnet\n",
            "    Found existing installation: mxnet 1.9.1\n",
            "    Uninstalling mxnet-1.9.1:\n",
            "      Successfully uninstalled mxnet-1.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime"
      ],
      "metadata": {
        "id": "Vl4SqEZxDCIP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 확인하기\n",
        "n_devices = torch.cuda.device_count()\n",
        "print(n_devices)\n",
        "\n",
        "for i in range(n_devices):\n",
        "    print(torch.cuda.get_device_name(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dmfy-PEDC3v",
        "outputId": "4b1e9c98-e67d-478c-8898-d4c7bce99b7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#코랩 환경에서 gpu 사용가능 여부를 판별 및 device 할당 위해서 사용\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "metadata": {
        "id": "Zjs4EEp2KenA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#코랩 환경에서 구글 드라이브 사용시 마운트 필요, 불필요 혹은 오류 발생시 주석 처리 후 실행\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z-3nedqDEPY",
        "outputId": "6613b842-7d82-4556-ad50-f2e35efed67e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "test = pd.read_csv('/drive/MyDrive/kor_bin_data/test_data.csv',encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "4_KJG5ltDI7Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_transformers import get_tokenizer\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('/drive/MyDrive/P_OR_N_MODEL')\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "Q4KNml3oLwOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 사용"
      ],
      "metadata": {
        "id": "2rOj5u-7KXKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#시그모이드\n",
        "def sigmoid(x): \n",
        "    return 1.0/(1 + np.exp(-x))\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "test_ans = [] #모델의 결과 저장\n",
        "test_prob = [] #가능성 저장\n",
        "\n",
        "#시작 시간 설정\n",
        "t0 = time.time()\n",
        "\n",
        "#테스트셋 데이터 전처리\n",
        "def convert_input_data(sentences):\n",
        "    sentences = [\"[CLS] \" + str(sentences) + \" [SEP]\"]\n",
        "\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # 입력 토큰의 최대 시퀀스 길이\n",
        "    MAX_LEN = 150\n",
        "\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    \n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # 데이터를 파이토치의 텐서로 변환\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, masks\n",
        "\n",
        "#모델 사용\n",
        "def Emotion_Binary_Classification(sentences):\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 문장을 입력 데이터로 변환\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # 데이터를 GPU에 넣음\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)\n",
        "            \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    \n",
        "    #시그모이드 함수, 확률 판단\n",
        "    pred = sigmoid(logits)\n",
        "    \n",
        "    #결과 판단\n",
        "    result = np.argmax(pred)\n",
        "\n",
        "    return [result,pred]\n"
      ],
      "metadata": {
        "id": "MeuqqfCgKloD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = input(\"분류할 문장을 입력하세요: \")\n",
        "a,p = Emotion_Binary_Classification(s)\n",
        "\n",
        "if a == 0:\n",
        "  print('\"',s,'\"',\"는 부정문입니다.\")\n",
        "else:\n",
        "  print('\"',s,'\"',\"는 긍정문입니다.\")  "
      ],
      "metadata": {
        "id": "nmiRH-qOKtaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 평가"
      ],
      "metadata": {
        "id": "rSupsPhoDOon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_transformers import get_tokenizer\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('/drive/MyDrive/P_OR_N_MODEL')\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "552tYJ4tD3Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도 계산 함수\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "    \n",
        "# 시간 표시 함수\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "5VFoTd0cDRCE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#시그모이드\n",
        "def sigmoid(x): \n",
        "    return 1.0/(1 + np.exp(-x))\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "test_ans = [] #모델의 결과 저장\n",
        "test_prob = [] #가능성 저장\n",
        "\n",
        "#시작 시간 설정\n",
        "t0 = time.time()\n",
        "\n",
        "#테스트셋 데이터 전처리\n",
        "def convert_input_data(sentences):\n",
        "    sentences = [\"[CLS] \" + str(sentences) + \" [SEP]\"]\n",
        "\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # 입력 토큰의 최대 시퀀스 길이\n",
        "    MAX_LEN = 150\n",
        "\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    \n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # 데이터를 파이토치의 텐서로 변환\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, masks\n",
        "\n",
        "#모델 사용\n",
        "def Determining_Acute_Ischemic_Stroke(sentences):\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 문장을 입력 데이터로 변환\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # 데이터를 GPU에 넣음\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)\n",
        "            \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    \n",
        "    #시그모이드 함수, 확률 판단\n",
        "    pred = sigmoid(logits)\n",
        "    \n",
        "    #결과 판단\n",
        "    result = np.argmax(pred)\n",
        "\n",
        "    return [result,pred]\n",
        "\n",
        "#이진 분류 결과(0 or 1)이 담길 정답 리스트\n",
        "test_ans = []\n",
        "\n",
        "#이진 분류 결과의 확률([0~1,0~1])이 담길 정답 리스트\n",
        "test_prob = []\n",
        "\n",
        "#입력 데이터\n",
        "d = test.Sentences\n",
        "d.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#실제 정답 \n",
        "labels = test.Emotions\n",
        "\n",
        "for i in range(len(d)):\n",
        "  #함수 호출, 이진분류 결과는 a, 확률은 p\n",
        "  a,p = Determining_Acute_Ischemic_Stroke(d[i])\n",
        "  test_ans.append(a) #결과 값은 test_ans에 저장\n",
        "  test_prob.append(p) #확률 값은 test_prob에 저장\n",
        "\n",
        "print(\"분류에 소요된 시간: {:}\".format(format_time(time.time() - t0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IncA1lgfDTNe",
        "outputId": "da366cd5-56e0-4b21-86a2-d28fa8945f8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "분류에 소요된 시간: 0:02:11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#probabilty중 정답이 1일 확률이 담길 리스트 \n",
        "t_prob =[]\n",
        "\n",
        "print('총 테스트 데이터의 수: ', len(test_prob))\n",
        "\n",
        "#probabilty중 정답이 1일 확률(model.predict_proba() 와 동일)\n",
        "for i in test_prob:\n",
        "  t_prob.append(i[0][1])\n",
        "\n",
        "cnt = 0\n",
        "for i in range(len(test_ans)):\n",
        "  if test_ans[i] == labels[i]:\n",
        "      cnt += 1\n",
        "\n",
        "print('Accuracy :',round(cnt/len(test_ans),6))      \n",
        "# roc_curve 그래프 그리기\n",
        "fpr, tpr, thresholds = roc_curve(labels, t_prob)\n",
        "\n",
        "roc = pd.DataFrame({'FPR(Fall-out)': fpr, 'TPRate(Recall)': tpr, 'Threshold': thresholds})\n",
        "\n",
        "plt.scatter(fpr, tpr)\n",
        "plt.title('model ROC curve')\n",
        "plt.xlabel('FPR(Fall-out)')\n",
        "plt.ylabel('TPR(Recall)');\n",
        "plt.plot(fpr, tpr, 'r--')\n",
        "\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "# 최적의 threshold\n",
        "#print('idx:',optimal_idx, 'threshold:', optimal_threshold)\n",
        "\n",
        "# AUC 면적 구하기\n",
        "auc_score = roc_auc_score(labels, t_prob)\n",
        "print('AUC Score:',round(auc_score,6))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "yMTy8HbMDWgH",
        "outputId": "f2e24363-3a84-4f89-96ed-6917933acfcf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 테스트 데이터의 수:  9113\n",
            "Accuracy : 0.898167\n",
            "AUC Score: 0.955392\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}